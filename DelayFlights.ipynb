{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios de Delay Flights para trabajar con dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear sesión\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"DelayFlightsPySpark\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar importamos los datos desde el csv de nuestra carpeta de descargas.  \n",
    "Ya que al inferir el Schema automaticamente puede dar problemas, lo añadimos manualmente.  \n",
    "Atención a la columna _date_, que la guardamos como un tipo string en vez de __Date__ o __timestamp__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "|01061215|   -6|     602|   ABE|        ATL|\n",
      "|01061725|   69|     602|   ABE|        ATL|\n",
      "|01061230|    0|     369|   ABE|        DTW|\n",
      "|01060625|   -3|     602|   ABE|        ATL|\n",
      "|01070600|    0|     369|   ABE|        DTW|\n",
      "|01071725|    0|     602|   ABE|        ATL|\n",
      "|01071230|    0|     369|   ABE|        DTW|\n",
      "|01070625|    0|     602|   ABE|        ATL|\n",
      "|01071219|    0|     569|   ABE|        ORD|\n",
      "|01080600|    0|     369|   ABE|        DTW|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importar datos\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "ruta = \"data/departuredelays.csv\"\n",
    "schema = (StructType()\n",
    "         .add(\"date\", StringType(), True)\n",
    "         .add(\"delay\", IntegerType(), True)\n",
    "         .add(\"distance\", IntegerType(), True)\n",
    "         .add(\"origin\", StringType(), True)\n",
    "         .add(\"destination\", StringType(), True))\n",
    "          \n",
    "df_raw = (spark.read.csv(ruta,\n",
    "                        header = True,\n",
    "                        schema = schema))\n",
    "df_raw.show()\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que queremos tratar la columna _date_ como si fueran fechas en vez de un string, la convertimos a formato. __timestamp__  \n",
    "* __Date__ solo incluye las partes de año, mes y día.  \n",
    "* __timestamp__ incluye año, mes, día, hora, minuto, segundo y milisegundo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------+------+-----------+\n",
      "|               date|delay|distance|origin|destination|\n",
      "+-------------------+-----+--------+------+-----------+\n",
      "|1970-01-01 12:45:00|    6|     602|   ABE|        ATL|\n",
      "|1970-01-02 06:00:00|   -8|     369|   ABE|        DTW|\n",
      "|1970-01-02 12:45:00|   -2|     602|   ABE|        ATL|\n",
      "|1970-01-02 06:05:00|   -4|     602|   ABE|        ATL|\n",
      "|1970-01-03 12:45:00|   -4|     602|   ABE|        ATL|\n",
      "|1970-01-03 06:05:00|    0|     602|   ABE|        ATL|\n",
      "|1970-01-04 12:43:00|   10|     602|   ABE|        ATL|\n",
      "|1970-01-04 06:05:00|   28|     602|   ABE|        ATL|\n",
      "|1970-01-05 12:45:00|   88|     602|   ABE|        ATL|\n",
      "|1970-01-05 06:05:00|    9|     602|   ABE|        ATL|\n",
      "|1970-01-06 12:15:00|   -6|     602|   ABE|        ATL|\n",
      "|1970-01-06 17:25:00|   69|     602|   ABE|        ATL|\n",
      "|1970-01-06 12:30:00|    0|     369|   ABE|        DTW|\n",
      "|1970-01-06 06:25:00|   -3|     602|   ABE|        ATL|\n",
      "|1970-01-07 06:00:00|    0|     369|   ABE|        DTW|\n",
      "|1970-01-07 17:25:00|    0|     602|   ABE|        ATL|\n",
      "|1970-01-07 12:30:00|    0|     369|   ABE|        DTW|\n",
      "|1970-01-07 06:25:00|    0|     602|   ABE|        ATL|\n",
      "|1970-01-07 12:19:00|    0|     569|   ABE|        ORD|\n",
      "|1970-01-08 06:00:00|    0|     369|   ABE|        DTW|\n",
      "+-------------------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cambiar a formato timestamp\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "df = (df_raw\n",
    "      .withColumn(\"date\", to_timestamp(\"date\", 'MMddHHmm').cast('timestamp')))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar diferentes funciones para obtener solo una parte de la fecha, esto resulta muy util a la hora de filtrar:  \n",
    "* __Year__ para obtener el año.\n",
    "* __Month__ para obtener el mes.\n",
    "* __Dayofmonth__ para obtener el día del mes.\n",
    "* __Dayofweek__ para obtener el día de la semana.\n",
    "* __Dayofyear__ para obtener el día del año.\n",
    "* __Hour__ para obtener la hora.\n",
    "* __Minute__ para obtener el minuto.\n",
    "* __Second__ para obtener el segundo y milisegundos si multiplicas por mil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------+------+-----------+\n",
      "|               date|delay|distance|origin|destination|\n",
      "+-------------------+-----+--------+------+-----------+\n",
      "|1970-02-01 12:15:00|   -8|     602|   ABE|        ATL|\n",
      "|1970-02-01 06:00:00|   -4|     369|   ABE|        DTW|\n",
      "|1970-02-01 06:25:00|   -1|     602|   ABE|        ATL|\n",
      "|1970-02-02 12:15:00|   -5|     602|   ABE|        ATL|\n",
      "|1970-02-02 17:25:00|   -9|     602|   ABE|        ATL|\n",
      "|1970-02-03 12:15:00|  158|     602|   ABE|        ATL|\n",
      "|1970-02-03 06:00:00|    8|     369|   ABE|        DTW|\n",
      "|1970-02-03 17:25:00|   76|     602|   ABE|        ATL|\n",
      "|1970-02-03 12:30:00|   30|     369|   ABE|        DTW|\n",
      "|1970-02-03 06:25:00|   22|     602|   ABE|        ATL|\n",
      "|1970-02-03 12:19:00|   30|     569|   ABE|        ORD|\n",
      "|1970-02-04 06:00:00|    6|     369|   ABE|        DTW|\n",
      "|1970-02-04 17:25:00|   -4|     602|   ABE|        ATL|\n",
      "|1970-02-04 12:30:00|   -2|     369|   ABE|        DTW|\n",
      "|1970-02-04 06:25:00|    5|     602|   ABE|        ATL|\n",
      "|1970-02-04 12:19:00|    0|     569|   ABE|        ORD|\n",
      "|1970-02-05 06:00:00|   91|     369|   ABE|        DTW|\n",
      "|1970-02-05 12:30:00|    0|     369|   ABE|        DTW|\n",
      "|1970-02-05 06:25:00|    0|     602|   ABE|        ATL|\n",
      "|1970-02-05 06:07:00|   46|     569|   ABE|        ORD|\n",
      "+-------------------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar por mes\n",
    "from pyspark.sql.functions import month\n",
    "df.filter(month(\"date\") == 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------+------+-----------+\n",
      "|               date|delay|distance|origin|destination|\n",
      "+-------------------+-----+--------+------+-----------+\n",
      "|1970-03-01 06:05:00|   -4|     369|   ABE|        DTW|\n",
      "|1970-03-01 06:25:00|   -6|     602|   ABE|        ATL|\n",
      "|1970-03-02 17:15:00|    1|     369|   ABE|        DTW|\n",
      "|1970-03-02 12:45:00|   -6|     369|   ABE|        DTW|\n",
      "|1970-03-02 17:25:00|  275|     602|   ABE|        ATL|\n",
      "|1970-03-02 06:00:00|   -2|     602|   ABE|        ATL|\n",
      "|1970-03-03 06:10:00|    0|     369|   ABE|        DTW|\n",
      "|1970-03-03 17:15:00|   86|     369|   ABE|        DTW|\n",
      "|1970-03-03 12:45:00|   -1|     369|   ABE|        DTW|\n",
      "|1970-03-03 12:06:00|   -2|     602|   ABE|        ATL|\n",
      "|1970-03-03 17:25:00|    0|     602|   ABE|        ATL|\n",
      "|1970-03-03 05:45:00|   -8|     602|   ABE|        ATL|\n",
      "|1970-03-03 16:28:00|    0|     569|   ABE|        ORD|\n",
      "|1970-03-04 06:10:00|    0|     369|   ABE|        DTW|\n",
      "|1970-03-04 17:15:00|   15|     369|   ABE|        DTW|\n",
      "|1970-03-04 12:45:00|   -8|     369|   ABE|        DTW|\n",
      "|1970-03-04 12:06:00|  -10|     602|   ABE|        ATL|\n",
      "|1970-03-04 17:25:00|   -1|     602|   ABE|        ATL|\n",
      "|1970-03-04 05:45:00|    5|     602|   ABE|        ATL|\n",
      "|1970-03-04 16:28:00|    0|     569|   ABE|        ORD|\n",
      "+-------------------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar por mes\n",
    "from pyspark.sql.functions import month\n",
    "df.filter(month(\"date\") > 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------+------+-----------+\n",
      "|               date|delay|distance|origin|destination|\n",
      "+-------------------+-----+--------+------+-----------+\n",
      "|1970-01-06 17:25:00|   69|     602|   ABE|        ATL|\n",
      "|1970-01-07 17:25:00|    0|     602|   ABE|        ATL|\n",
      "|1970-01-09 17:25:00|    0|     602|   ABE|        ATL|\n",
      "|1970-01-10 17:25:00|    7|     602|   ABE|        ATL|\n",
      "|1970-01-12 17:25:00|   -1|     602|   ABE|        ATL|\n",
      "|1970-01-13 17:25:00|   -6|     602|   ABE|        ATL|\n",
      "|1970-01-14 17:25:00|   -9|     602|   ABE|        ATL|\n",
      "|1970-01-15 17:25:00|   -6|     602|   ABE|        ATL|\n",
      "|1970-01-16 17:25:00|   -6|     602|   ABE|        ATL|\n",
      "|1970-01-17 17:25:00|    5|     602|   ABE|        ATL|\n",
      "|1970-01-19 17:25:00|   -5|     602|   ABE|        ATL|\n",
      "|1970-01-20 17:25:00|   -5|     602|   ABE|        ATL|\n",
      "|1970-01-21 17:25:00|    0|     602|   ABE|        ATL|\n",
      "|1970-01-23 17:25:00|  180|     602|   ABE|        ATL|\n",
      "|1970-01-24 17:25:00|    2|     602|   ABE|        ATL|\n",
      "|1970-01-26 17:25:00|    7|     602|   ABE|        ATL|\n",
      "|1970-01-27 17:25:00|   -8|     602|   ABE|        ATL|\n",
      "|1970-01-28 17:25:00|   -3|     602|   ABE|        ATL|\n",
      "|1970-01-30 17:25:00|   10|     602|   ABE|        ATL|\n",
      "|1970-01-31 17:25:00|   25|     602|   ABE|        ATL|\n",
      "+-------------------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar por hora\n",
    "from pyspark.sql.functions import hour\n",
    "df.filter(hour(\"date\") > 12).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se pueden sumar valores a la fecha, incluso de manera dinámica.  \n",
    "* __unix_timestamp__ convierte del formato timestamp a los segundos pasados desde el \"epoch\" (01/01/1970) lo cual nos permite trabajar con las fechas.  \n",
    "* atención con el _delay_, que está en minutos y no en segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------+------+-----------+-------------------+\n",
      "|               date|delay|distance|origin|destination|       delayed_date|\n",
      "+-------------------+-----+--------+------+-----------+-------------------+\n",
      "|1970-01-01 12:45:00|    6|     602|   ABE|        ATL|1970-01-01 12:51:00|\n",
      "|1970-01-02 06:00:00|   -8|     369|   ABE|        DTW|1970-01-02 05:52:00|\n",
      "|1970-01-02 12:45:00|   -2|     602|   ABE|        ATL|1970-01-02 12:43:00|\n",
      "|1970-01-02 06:05:00|   -4|     602|   ABE|        ATL|1970-01-02 06:01:00|\n",
      "|1970-01-03 12:45:00|   -4|     602|   ABE|        ATL|1970-01-03 12:41:00|\n",
      "|1970-01-03 06:05:00|    0|     602|   ABE|        ATL|1970-01-03 06:05:00|\n",
      "|1970-01-04 12:43:00|   10|     602|   ABE|        ATL|1970-01-04 12:53:00|\n",
      "|1970-01-04 06:05:00|   28|     602|   ABE|        ATL|1970-01-04 06:33:00|\n",
      "|1970-01-05 12:45:00|   88|     602|   ABE|        ATL|1970-01-05 14:13:00|\n",
      "|1970-01-05 06:05:00|    9|     602|   ABE|        ATL|1970-01-05 06:14:00|\n",
      "|1970-01-06 12:15:00|   -6|     602|   ABE|        ATL|1970-01-06 12:09:00|\n",
      "|1970-01-06 17:25:00|   69|     602|   ABE|        ATL|1970-01-06 18:34:00|\n",
      "|1970-01-06 12:30:00|    0|     369|   ABE|        DTW|1970-01-06 12:30:00|\n",
      "|1970-01-06 06:25:00|   -3|     602|   ABE|        ATL|1970-01-06 06:22:00|\n",
      "|1970-01-07 06:00:00|    0|     369|   ABE|        DTW|1970-01-07 06:00:00|\n",
      "|1970-01-07 17:25:00|    0|     602|   ABE|        ATL|1970-01-07 17:25:00|\n",
      "|1970-01-07 12:30:00|    0|     369|   ABE|        DTW|1970-01-07 12:30:00|\n",
      "|1970-01-07 06:25:00|    0|     602|   ABE|        ATL|1970-01-07 06:25:00|\n",
      "|1970-01-07 12:19:00|    0|     569|   ABE|        ORD|1970-01-07 12:19:00|\n",
      "|1970-01-08 06:00:00|    0|     369|   ABE|        DTW|1970-01-08 06:00:00|\n",
      "+-------------------+-----+--------+------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Añadir dinamicamente el delay a date\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "dfDelays = (df.withColumn(\"delayed_date\", (unix_timestamp(df.date) + df.delay * 60).cast('timestamp')))\n",
    "dfDelays.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe también la opción de solo mostrar ciertas partes del __date__ o cambiar su estructura.  \n",
    "Esto viene bien si hay ciertas partes que no aportan información o queremos deshacernos de ellas (como aquí con el año y los segundos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+------+-----------+\n",
      "|       date|delay|distance|origin|destination|\n",
      "+-----------+-----+--------+------+-----------+\n",
      "|01-01 12:45|    6|     602|   ABE|        ATL|\n",
      "|01-02 06:00|   -8|     369|   ABE|        DTW|\n",
      "|01-02 12:45|   -2|     602|   ABE|        ATL|\n",
      "|01-02 06:05|   -4|     602|   ABE|        ATL|\n",
      "|01-03 12:45|   -4|     602|   ABE|        ATL|\n",
      "|01-03 06:05|    0|     602|   ABE|        ATL|\n",
      "|01-04 12:43|   10|     602|   ABE|        ATL|\n",
      "|01-04 06:05|   28|     602|   ABE|        ATL|\n",
      "|01-05 12:45|   88|     602|   ABE|        ATL|\n",
      "|01-05 06:05|    9|     602|   ABE|        ATL|\n",
      "|01-06 12:15|   -6|     602|   ABE|        ATL|\n",
      "|01-06 17:25|   69|     602|   ABE|        ATL|\n",
      "|01-06 12:30|    0|     369|   ABE|        DTW|\n",
      "|01-06 06:25|   -3|     602|   ABE|        ATL|\n",
      "|01-07 06:00|    0|     369|   ABE|        DTW|\n",
      "|01-07 17:25|    0|     602|   ABE|        ATL|\n",
      "|01-07 12:30|    0|     369|   ABE|        DTW|\n",
      "|01-07 06:25|    0|     602|   ABE|        ATL|\n",
      "|01-07 12:19|    0|     569|   ABE|        ORD|\n",
      "|01-08 06:00|    0|     369|   ABE|        DTW|\n",
      "+-----------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar solo partes del date\n",
    "from pyspark.sql.functions import date_format\n",
    "df_clean = (df.select(date_format(df.date, 'MM-dd HH:mm').alias(\"date\"), df.delay, df.distance, df.origin, df.destination))\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+------+-----------+------------+\n",
      "|       date|delay|distance|origin|destination|delayed_date|\n",
      "+-----------+-----+--------+------+-----------+------------+\n",
      "|01-01 12:45|    6|     602|   ABE|        ATL| 01-01 12:51|\n",
      "|01-02 06:00|   -8|     369|   ABE|        DTW| 01-02 05:52|\n",
      "|01-02 12:45|   -2|     602|   ABE|        ATL| 01-02 12:43|\n",
      "|01-02 06:05|   -4|     602|   ABE|        ATL| 01-02 06:01|\n",
      "|01-03 12:45|   -4|     602|   ABE|        ATL| 01-03 12:41|\n",
      "|01-03 06:05|    0|     602|   ABE|        ATL| 01-03 06:05|\n",
      "|01-04 12:43|   10|     602|   ABE|        ATL| 01-04 12:53|\n",
      "|01-04 06:05|   28|     602|   ABE|        ATL| 01-04 06:33|\n",
      "|01-05 12:45|   88|     602|   ABE|        ATL| 01-05 14:13|\n",
      "|01-05 06:05|    9|     602|   ABE|        ATL| 01-05 06:14|\n",
      "|01-06 12:15|   -6|     602|   ABE|        ATL| 01-06 12:09|\n",
      "|01-06 17:25|   69|     602|   ABE|        ATL| 01-06 18:34|\n",
      "|01-06 12:30|    0|     369|   ABE|        DTW| 01-06 12:30|\n",
      "|01-06 06:25|   -3|     602|   ABE|        ATL| 01-06 06:22|\n",
      "|01-07 06:00|    0|     369|   ABE|        DTW| 01-07 06:00|\n",
      "|01-07 17:25|    0|     602|   ABE|        ATL| 01-07 17:25|\n",
      "|01-07 12:30|    0|     369|   ABE|        DTW| 01-07 12:30|\n",
      "|01-07 06:25|    0|     602|   ABE|        ATL| 01-07 06:25|\n",
      "|01-07 12:19|    0|     569|   ABE|        ORD| 01-07 12:19|\n",
      "|01-08 06:00|    0|     369|   ABE|        DTW| 01-08 06:00|\n",
      "+-----------+-----+--------+------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Añadir delayed_date y mostrar solo la información importante\n",
    "from pyspark.sql.functions import date_format\n",
    "dfDelays_clean = (dfDelays.select(date_format(dfDelays.date, 'MM-dd HH:mm').alias(\"date\"),\n",
    "                                  dfDelays.delay,\n",
    "                                  dfDelays.distance,\n",
    "                                  df.origin,\n",
    "                                  dfDelays.destination,\n",
    "                                  date_format(dfDelays.delayed_date, 'MM-dd HH:mm').alias(\"delayed_date\")))\n",
    "dfDelays_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo modo que podíamos sumar numeros al __date__, tenemos la opción de restar dates para obtener valores.  \n",
    "  \n",
    "Esto es tan sencillo como convertir el formato __date__/__timestamp__ a un valor numérico y operar con él.  \n",
    "Antes para ello hemos utilizado __unix_timestamp__ pero también podemos hacer un cast a __long__, por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+------+\n",
      "|               date|       delayed_date|delay|delay2|\n",
      "+-------------------+-------------------+-----+------+\n",
      "|1970-01-01 12:45:00|1970-01-01 12:51:00|    6|     6|\n",
      "|1970-01-02 06:00:00|1970-01-02 05:52:00|   -8|    -8|\n",
      "|1970-01-02 12:45:00|1970-01-02 12:43:00|   -2|    -2|\n",
      "|1970-01-02 06:05:00|1970-01-02 06:01:00|   -4|    -4|\n",
      "|1970-01-03 12:45:00|1970-01-03 12:41:00|   -4|    -4|\n",
      "|1970-01-03 06:05:00|1970-01-03 06:05:00|    0|     0|\n",
      "|1970-01-04 12:43:00|1970-01-04 12:53:00|   10|    10|\n",
      "|1970-01-04 06:05:00|1970-01-04 06:33:00|   28|    28|\n",
      "|1970-01-05 12:45:00|1970-01-05 14:13:00|   88|    88|\n",
      "|1970-01-05 06:05:00|1970-01-05 06:14:00|    9|     9|\n",
      "|1970-01-06 12:15:00|1970-01-06 12:09:00|   -6|    -6|\n",
      "|1970-01-06 17:25:00|1970-01-06 18:34:00|   69|    69|\n",
      "|1970-01-06 12:30:00|1970-01-06 12:30:00|    0|     0|\n",
      "|1970-01-06 06:25:00|1970-01-06 06:22:00|   -3|    -3|\n",
      "|1970-01-07 06:00:00|1970-01-07 06:00:00|    0|     0|\n",
      "|1970-01-07 17:25:00|1970-01-07 17:25:00|    0|     0|\n",
      "|1970-01-07 12:30:00|1970-01-07 12:30:00|    0|     0|\n",
      "|1970-01-07 06:25:00|1970-01-07 06:25:00|    0|     0|\n",
      "|1970-01-07 12:19:00|1970-01-07 12:19:00|    0|     0|\n",
      "|1970-01-08 06:00:00|1970-01-08 06:00:00|    0|     0|\n",
      "+-------------------+-------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Volver a sacar el delay con los 2 dates\n",
    "from pyspark.sql.functions import datediff\n",
    "(dfDelays.select(dfDelays.date,\n",
    "                 dfDelays.delayed_date,\n",
    "                 dfDelays.delay,\n",
    "                 ((dfDelays.delayed_date.cast(\"long\") - dfDelays.date.cast(\"long\")) / 60).cast(\"int\").alias(\"delay2\"))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|total_delay|\n",
      "+------+-----------+-----------+\n",
      "|   ABE|        ATL|       2748|\n",
      "|   ABE|        DTW|       1158|\n",
      "|   ABE|        ORD|       1207|\n",
      "|   ABI|        DFW|       5128|\n",
      "|   ABQ|        ATL|       1852|\n",
      "|   ABQ|        BWI|        695|\n",
      "|   ABQ|        DAL|       8610|\n",
      "|   ABQ|        DEN|       4135|\n",
      "|   ABQ|        DFW|       3966|\n",
      "|   ABQ|        HOU|       3539|\n",
      "|   ABQ|        IAD|        280|\n",
      "|   ABQ|        IAH|       2926|\n",
      "|   ABQ|        JFK|        789|\n",
      "|   ABQ|        LAS|       6151|\n",
      "|   ABQ|        LAX|       8085|\n",
      "|   ABQ|        MCI|        888|\n",
      "|   ABQ|        MDW|       2342|\n",
      "|   ABQ|        MSP|         47|\n",
      "|   ABQ|        OAK|       3971|\n",
      "|   ABQ|        ORD|       1580|\n",
      "+------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el delay total por origen y destino\n",
    "df_grouped_od = (df.select(\"origin\", \"destination\", \"delay\")\n",
    "             .groupBy(\"origin\", \"destination\")\n",
    "             .sum(\"delay\")\n",
    "             .withColumnRenamed(\"sum(delay)\", \"total_delay\")\n",
    "             .orderBy(\"origin\", \"destination\"))\n",
    "\n",
    "df_grouped_od.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "|month|day|total_delay|\n",
      "+-----+---+-----------+\n",
      "|    1|  1|     -26079|\n",
      "|    1|  2|     -19373|\n",
      "|    1|  3|     -17628|\n",
      "|    1|  4|     -13464|\n",
      "|    1|  5|     -12640|\n",
      "|    1|  6|     -13071|\n",
      "|    1|  7|     -15629|\n",
      "|    1|  8|     -20911|\n",
      "|    1|  9|     -28199|\n",
      "|    1| 10|     -29608|\n",
      "|    1| 11|     -25413|\n",
      "|    1| 12|     -39531|\n",
      "|    1| 13|     -49322|\n",
      "|    1| 14|     -49566|\n",
      "|    1| 15|     -44655|\n",
      "|    1| 16|     -39108|\n",
      "|    1| 17|     -38876|\n",
      "|    1| 18|     -34209|\n",
      "|    1| 19|     -46473|\n",
      "|    1| 20|     -45128|\n",
      "+-----+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- total_delay: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el delay total por dia y mes\n",
    "from pyspark.sql.functions import month, dayofmonth\n",
    "df_grouped_dm = (df.select(month(\"date\").alias(\"month\"),\n",
    "                           dayofmonth(\"date\").alias(\"day\"),\n",
    "                           \"delay\")\n",
    "                 .filter(df.delay < 0)\n",
    "             .groupBy(\"month\", \"day\")\n",
    "             .sum(\"delay\")\n",
    "             .withColumnRenamed(\"sum(delay)\", \"total_delay\")\n",
    "             .orderBy(\"month\", \"day\"))\n",
    "\n",
    "df_grouped_dm.show()\n",
    "df_grouped_dm.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
